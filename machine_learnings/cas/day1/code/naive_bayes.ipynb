{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encoding:utf-8\n",
    "\n",
    "# 朴素贝叶斯分类器\n",
    "\n",
    "import math, random\n",
    "import numpy as np\n",
    "from util.file_utils import *\n",
    "from util.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_loader(data_dir):\n",
    "    instances, labels = list(), list()\n",
    "    for label in os.listdir(data_dir):\n",
    "        sub_dir = os.path.join(data_dir, label)\n",
    "        for name in os.listdir(sub_dir):\n",
    "            one_line = '\\n'.join(read_lines(os.path.join(sub_dir, name), 'gbk'))\n",
    "            words = tokenizer(one_line, 'zh')\n",
    "            instances.append(words)\n",
    "            labels.append(label)\n",
    "    total_instances = zip(instances, labels)\n",
    "    random.shuffle(total_instances)\n",
    "    inst_num_part = int(len(total_instances) * 0.1)\n",
    "    return total_instances[:7*inst_num_part], total_instances[7*inst_num_part:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_vocab(instances):\n",
    "    labels, label2inds = list(), dict()\n",
    "    words, word2inds = list(), dict()\n",
    "    for tokens, label in instances:\n",
    "        if label not in label2inds:\n",
    "            label2inds[label] = len(labels)\n",
    "            labels.append(label)\n",
    "        for token in tokens:\n",
    "            if token not in word2inds:\n",
    "                word2inds[token] = len(words)\n",
    "                words.append(token)\n",
    "    return words, word2inds, labels, label2inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(train_insts, words, word2inds, labels, label2inds):\n",
    "    label_freqs = np.zeros(len(labels)) # 20\n",
    "    label_token_freqs = np.zeros((len(labels), len(words))) # 20, 20000\n",
    "\n",
    "    for tokens, label in train_insts:\n",
    "        label_id = label2inds[label]\n",
    "        label_freqs[label_id] += 1\n",
    "        for token in tokens:\n",
    "            token_id = word2inds[token]\n",
    "            label_token_freqs[label_id, token_id] += 1\n",
    "\n",
    "    label_token_freq_sum = np.sum(label_token_freqs, 1)\n",
    "\n",
    "    # model_pc = label_freqs * 1.0 / np.sum(label_freqs)\n",
    "    # model_pw = np.transpose(label_token_freqs) * 1.0 / label_token_freq_sum\n",
    "    model_pc = (label_freqs + 1.0) / (np.sum(label_freqs) + len(labels))\n",
    "    model_pw = np.transpose(label_token_freqs + 1.0) / (label_token_freq_sum + len(words))\n",
    "    model_pw = np.transpose(model_pw)\n",
    "\n",
    "    return model_pc, model_pw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify(tokens, classifier, words, word2inds, labels, label2inds):\n",
    "    # model包括两部分概率，一部分是类别的先验概率，另一部分是类别的概率（生成）特征的概率\n",
    "    model_pc, model_pw = classifier # 是numpy多维数据\n",
    "    probs = list() # 该示例对应不同类别的概率\n",
    "    for label in labels:\n",
    "        label_id = label2inds[label]\n",
    "        prob = model_pc[label_id] # 先验概率\n",
    "        word_prob_sum = 0\n",
    "        for token in tokens:\n",
    "            if token not in word2inds:\n",
    "                continue\n",
    "            token_id = word2inds[token]\n",
    "            word_prob_sum += math.log(model_pw[label_id, token_id])\n",
    "        probs.append(math.log(prob) + word_prob_sum)\n",
    "    probs_np = np.asarray(probs)\n",
    "    label_ind = np.argmax(probs_np)\n",
    "    return labels[label_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 1.765 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  88.671875\n"
     ]
    }
   ],
   "source": [
    "dir_name = '/home/dl/codes/day1/tc-corpus-answer-test/'\n",
    "train_insts, test_insts = data_loader(dir_name)\n",
    "words, word2inds, labels, label2inds = init_vocab(train_insts)\n",
    "classifier = train(train_insts, words, word2inds, labels, label2inds)\n",
    "accs = list()\n",
    "for words, real_label in test_insts:\n",
    "    pred_label = classify(words, classifier, words, word2inds, labels, label2inds)\n",
    "    if pred_label == real_label:\n",
    "        accs.append(1)\n",
    "    else:\n",
    "        accs.append(0)\n",
    "print \"accuracy: \", sum(accs) * 100.0 / len(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
